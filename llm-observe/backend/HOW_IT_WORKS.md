# How Function-Level Workflow Tracing Works

## Overview

Yes, this will work for **all your agent calls, API calls, etc.** Here's how:

## How It Works

### 1. **API Calls (OpenAI, Pinecone, etc.)**
âœ… **Already working** - Your existing instrumentors (OpenAI, Pinecone, etc.) automatically create spans for API calls.

### 2. **Function-Level Workflow Spans**
âœ… **Now working** - Functions are wrapped to create workflow spans that group all API calls within that function.

### 3. **The Flow**

```
User calls simulate_agent_workflow()
  â†“
@workflow_trace decorator creates: workflow.simulate_agent_workflow (root span)
  â†“
First API call: openai.chat.completions.create()
  â†“
ensure_root_span("openai") checks for workflow span â†’ finds it!
  â†“
Creates: llm.request (child of workflow.simulate_agent_workflow)
  â†“
Second API call: pinecone.Index().query()
  â†“
ensure_root_span("pinecone") checks for workflow span â†’ finds it!
  â†“
Creates: pinecone.query (child of workflow.simulate_agent_workflow)
  â†“
Third API call: openai.chat.completions.create()
  â†“
Creates: llm.request (child of workflow.simulate_agent_workflow)
  â†“
Function ends â†’ workflow span closes
```

## What Gets Traced

### âœ… **Automatically Traced (No Code Changes Needed)**

1. **All API calls** via instrumentors:
   - OpenAI (`openai.chat.completions.create()`)
   - Pinecone (`index.query()`, `index.upsert()`, etc.)
   - Anthropic, Cohere, Mistral, Gemini, etc. (if instrumentors installed)

2. **Functions with `@workflow_trace` decorator**:
   - `simulate_agent_workflow()` âœ… (already wrapped)
   - `run_fake_app()` âœ… (already wrapped)
   - Any function you add `@workflow_trace` to

3. **Functions matching patterns** (if import hook works):
   - Functions ending in `_workflow`, `_agent`, `_handler`
   - Configurable via `LLMOBSERVE_FUNCTION_PATTERNS`

### âš ï¸ **Manual Wrapping Required**

For functions that are imported before `llmobserve` is imported, you need to manually add `@workflow_trace`:

```python
from llmobserve.tracing.function_tracer import workflow_trace

@workflow_trace
async def my_agent_function():
    # All API calls here will be children of workflow.my_agent_function
    response = openai.chat.completions.create(...)
    return response
```

## Trace Structure

### Before (Without Function Tracing)
```
auto.workflow.openai (root)
  â””â”€â”€ llm.request (child)

auto.workflow.pinecone (root)
  â””â”€â”€ pinecone.query (child)

auto.workflow.openai (root)
  â””â”€â”€ llm.request (child)
```
**Problem**: Each API call creates its own root span - no grouping!

### After (With Function Tracing)
```
workflow.simulate_agent_workflow (root)
  â”œâ”€â”€ llm.request (child) - GPT query generation
  â”œâ”€â”€ pinecone.query (child) - Vector search
  â””â”€â”€ llm.request (child) - GPT summarization
```
**Solution**: All API calls grouped under one workflow span!

## Example: Your Current Code

### `simulate_agent_workflow()` âœ…

```python
@workflow_trace  # â† Creates workflow span
async def simulate_agent_workflow() -> dict:
    # Workflow span: workflow.simulate_agent_workflow
    
    # API call 1 â†’ llm.request (child)
    query_resp = await client.chat.completions.create(...)
    
    # API call 2 â†’ pinecone.query (child)
    results = await index.query(...)
    
    # API call 3 â†’ llm.request (child)
    summary_resp = await client.chat.completions.create(...)
    
    return result
```

**Result**: All 3 API calls are grouped under `workflow.simulate_agent_workflow`!

## Edge Cases Handled

âœ… **Nested functions**: Inner functions inherit parent's workflow span
âœ… **Async functions**: Context propagates via `contextvars`
âœ… **Threading**: Context copied to threads
âœ… **Exceptions**: Workflow span always ends (finally block)
âœ… **Recursive functions**: Each call gets its own workflow span
âœ… **Functions with no API calls**: Still get workflow spans (for visibility)

## Configuration

Set these environment variables to control behavior:

```bash
# Enable/disable function tracing (default: true)
LLMOBSERVE_AUTO_FUNCTION_TRACING=true

# Function name patterns to wrap (default: *_workflow,*_agent,*_handler)
LLMOBSERVE_FUNCTION_PATTERNS="*_workflow,*_agent,*_handler"

# Modules to exclude (default: test,__pycache__)
LLMOBSERVE_EXCLUDE_MODULES="test,__pycache__,migrations"
```

## Testing

To verify it's working:

1. **Call your agent function**:
   ```bash
   curl -X POST http://localhost:8000/demo/simulate-agent
   ```

2. **Check the trace**:
   - Should see `workflow.simulate_agent_workflow` as root span
   - Should see 3 child spans: `llm.request`, `pinecone.query`, `llm.request`
   - All should have the same `trace_id`

3. **Check the dashboard**:
   - Workflow should appear in the workflows table
   - Trace tree should show the hierarchy

## Summary

âœ… **API calls**: Automatically traced (no changes needed)
âœ… **Agent functions**: Wrapped with `@workflow_trace` decorator
âœ… **Grouping**: All API calls within a function are grouped under one workflow span
âœ… **Context propagation**: Works across async, threading, nested calls
âœ… **Edge cases**: Handled (exceptions, recursion, etc.)

**Bottom line**: Yes, this will work for all your agent calls and API calls! ðŸŽ‰


